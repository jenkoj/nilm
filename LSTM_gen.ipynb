{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.8/dist-packages (0.0)\n",
      "Requirement already satisfied: pyts in /usr/local/lib/python3.8/dist-packages (0.11.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.4.3)\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.8/dist-packages (1.1.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sklearn) (1.0)\n",
      "Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.8/dist-packages (from pyts) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pyts) (1.4.1)\n",
      "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.8/dist-packages (from pyts) (0.54.1)\n",
      "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.8/dist-packages (from pyts) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (4.5.3.56)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from albumentations) (5.4.1)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (0.18.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /usr/local/lib/python3.8/dist-packages (from numba>=0.48.0->pyts) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.48.0->pyts) (57.4.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from qudida>=0.0.4->albumentations) (3.7.4.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.10.12)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (3.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn pyts matplotlib albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%jupyter` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "WqUfHWgtJxtu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools \n",
    "import pyts\n",
    "from pyts.approximation import PiecewiseAggregateApproximation\n",
    "\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.layers import Convolution2D,Conv2D, Dense,Dropout, Flatten, Activation, MaxPooling2D, Input, Conv1D, GlobalAveragePooling1D, TimeDistributed, GRU, LSTM\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "AYrWi4wMKCWE"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_norm(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    This function prints and plots the confusion matrix.\n",
    "\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        cm = np.around(cm, decimals=2)\n",
    "\n",
    "        cm[np.isnan(cm)] = 0.0\n",
    "\n",
    "        print(\"Normalized confusion matrix\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "\n",
    "        plt.text(j, i, cm[i, j],\n",
    "\n",
    "                 horizontalalignment=\"center\",\n",
    "\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "h5_dMKrLKHJt"
   },
   "outputs": [],
   "source": [
    "def LSTM_3D(X_train, num_classes, lr = 0.0001):\n",
    "\n",
    "    input_a = Input(shape = X_train.shape[1: ])\n",
    "\n",
    "    x = TimeDistributed(Convolution2D(16, (7, 7), activation='relu', padding='same'))(input_a) //\n",
    "    x = TimeDistributed(Convolution2D(16, (7, 7), activation='relu', padding='same'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2, 2), padding='same'))(x)\n",
    "    x = TimeDistributed(Convolution2D(16, (3, 3), activation='relu', padding='same'))(x)\n",
    "    x = TimeDistributed(Convolution2D(16, (3, 3), activation='relu', padding='same'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2, 2), padding='same'))(x)\n",
    "    x = TimeDistributed(Convolution2D(16, (3, 3), activation='relu', padding='same'))(x)\n",
    "    x = TimeDistributed(Convolution2D(16, (3, 3), activation='relu', padding='same'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2, 2), padding='same'))(x)\n",
    "    x = Dropout(0.10)(x)\n",
    "\n",
    "    out = TimeDistributed(Convolution2D(16, (3, 3), activation='relu', padding='same'))(x)\n",
    "    out = TimeDistributed(Convolution2D(16, (3, 3), activation='relu', padding='same'))(out)\n",
    "        # out = TimeDistributed(MaxPooling2D(pool_size=(2, 2), padding='same'))(out)\n",
    "        # out = Dropout(0.60)(out)\n",
    "    out = TimeDistributed(Flatten())(out)\n",
    "    out = LSTM(16, return_sequences=False, unroll=False, dropout=0.2)(out)  # dropout=0.6 #return sequences = True\n",
    "    prediction = Dense(num_classes, activation='softmax')(out)\n",
    "\n",
    "    model = keras.models.Model(inputs=input_a, outputs=prediction)\n",
    "    adam = optimizers.Adam(lr = lr)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "leuzbmn-KODX"
   },
   "outputs": [],
   "source": [
    "def basic_cnn_functional(X_train, num_classes, lr = 0.0001):\n",
    "    \n",
    "    input_a = Input(shape = X_train.shape[1: ])\n",
    "    \n",
    "    x = Convolution2D(16, (7, 7), activation='relu', padding='same')(input_a)\n",
    "    x = Convolution2D(16, (7, 7), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "    x = Convolution2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Convolution2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "    x = Convolution2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Convolution2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "    x = Dropout(0.10)(x)\n",
    "\n",
    "    out = Convolution2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    out = Convolution2D(16, (3, 3), activation='relu', padding='same')(out)\n",
    "        # out = TimeDistributed(MaxPooling2D(pool_size=(2, 2), padding='same'))(out)\n",
    "        # out = Dropout(0.60)(out)\n",
    "    out = Flatten()(out)\n",
    "    # dense layer with 50 neurons\n",
    "    dense = Dense(64, activation = 'relu')(out)\n",
    "    # final layer with 10 neurons to classify the instances\n",
    "    output = Dense(num_classes, activation = 'softmax')(dense)\n",
    "    \n",
    "    adam = optimizers.Adam(lr = lr)\n",
    "    model = keras.models.Model(inputs=input_a, outputs=output)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "RT1vwq7-KOtB"
   },
   "outputs": [],
   "source": [
    "#LOAD DATASET\n",
    "file_name = \"refit_gen_GAF_13m_100S5X_22A147955_R12_S80-20\"\n",
    "\n",
    "#read the file\n",
    "path = str(pathlib.Path().resolve())\n",
    "\n",
    "path_data = f\"{path}\"\"/data/\"f\"{file_name}\"\"/\"f\"{file_name}\"\".hdf5\"\n",
    "\n",
    "file = h5py.File(path_data,\"r+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = file[\"data/train/gaf\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100, 100, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "0IzkR99fKbRe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['breadmaker', 'toaster', 'tumble dryer', 'audio system', 'food processor', 'fridge', 'games console', 'fridge freezer', 'kettle', 'broadband router', 'appliance', 'washer dryer', 'television', 'electric space heater', 'fan', 'dish washer', 'pond pump', 'microwave', 'unknown', 'computer', 'washing machine', 'dehumidifier']\n"
     ]
    }
   ],
   "source": [
    "#fetch array of appliances\n",
    "enc_appliances = np.array(file[\"classes/appliances\"])\n",
    "appliances = [n.decode(\"utf-8\") for n in enc_appliances]\n",
    "num_of_classes = file[\"classes/appliances\"].shape[0]\n",
    "#manualy_selected_appliances.remove(\"microwave\")\n",
    "\n",
    "#fetch array of weights\n",
    "#class_weights = np.array(file[\"classes/weights\"])\n",
    "#class_weights = np.delete(class_weights,5)\n",
    "print(appliances)\n",
    "\n",
    "#print(class_weights)\n",
    "\n",
    "# #read train and label data\n",
    "#data_train = np.array(file['data/train']['gasf'])\n",
    "\n",
    "# labels_test = np.array(file['labels/test']['gasf'])\n",
    "# labels_train = np.array(file['labels/train']['gasf'])\n",
    "\n",
    "# X_train = np.array(file['data/train']['gaf'])\n",
    "# X_test = np.array(file['data/test']['gaf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "5S-dku5WKpQx"
   },
   "outputs": [],
   "source": [
    "#read the file\n",
    "\n",
    "train_gen = HDF5ImageGenerator(\n",
    "    src= path_data,\n",
    "    X_key=\"data/train/gaf\",\n",
    "    y_key=\"labels/train/gasf\",\n",
    "    classes_key=\"classes/appliances\",\n",
    "    labels_encoding=\"hot\",\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    num_classes=num_of_classes,\n",
    "    mode=\"train\"\n",
    "    )\n",
    "\n",
    "test_gen = HDF5ImageGenerator(\n",
    "    src= path,\n",
    "    scaler=False,\n",
    "    X_key=\"data/test/gasf\",\n",
    "    y_key=\"labels/test/gaf\",\n",
    "    classes_key=\"classes/appliances\",\n",
    "    labels_encoding=\"hot\",\n",
    "    shuffle=False,\n",
    "    batch_size=32,\n",
    "    num_classes=num_of_classes,\n",
    "    mode=\"train\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "vqA9NwOqKrb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4543/4543 - 145s - loss: 2.1142 - accuracy: 0.4446\n",
      "Epoch 2/150\n",
      "4543/4543 - 145s - loss: 1.6271 - accuracy: 0.5628\n",
      "Epoch 3/150\n",
      "4543/4543 - 145s - loss: 1.4301 - accuracy: 0.5948\n",
      "Epoch 4/150\n",
      "4543/4543 - 145s - loss: 1.2952 - accuracy: 0.6238\n",
      "Epoch 5/150\n",
      "4543/4543 - 144s - loss: 1.2020 - accuracy: 0.6404\n",
      "Epoch 6/150\n",
      "4543/4543 - 145s - loss: 1.1219 - accuracy: 0.6606\n",
      "Epoch 7/150\n",
      "4543/4543 - 144s - loss: 1.0586 - accuracy: 0.6798\n",
      "Epoch 8/150\n",
      "4543/4543 - 146s - loss: 1.0102 - accuracy: 0.6939\n",
      "Epoch 9/150\n",
      "4543/4543 - 145s - loss: 0.9702 - accuracy: 0.7037\n",
      "Epoch 10/150\n",
      "4543/4543 - 145s - loss: 0.9315 - accuracy: 0.7120\n",
      "Epoch 11/150\n",
      "4543/4543 - 145s - loss: 0.8954 - accuracy: 0.7202\n",
      "Epoch 12/150\n",
      "4543/4543 - 144s - loss: 0.8648 - accuracy: 0.7257\n",
      "Epoch 13/150\n",
      "4543/4543 - 145s - loss: 0.8414 - accuracy: 0.7314\n",
      "Epoch 14/150\n",
      "4543/4543 - 145s - loss: 0.8192 - accuracy: 0.7363\n",
      "Epoch 15/150\n",
      "4543/4543 - 145s - loss: 0.7994 - accuracy: 0.7423\n",
      "Epoch 16/150\n",
      "4543/4543 - 144s - loss: 0.7830 - accuracy: 0.7470\n",
      "Epoch 17/150\n",
      "4543/4543 - 144s - loss: 0.7673 - accuracy: 0.7512\n",
      "Epoch 18/150\n",
      "4543/4543 - 144s - loss: 0.7536 - accuracy: 0.7558\n",
      "Epoch 19/150\n",
      "4543/4543 - 145s - loss: 0.7402 - accuracy: 0.7593\n",
      "Epoch 20/150\n",
      "4543/4543 - 145s - loss: 0.7287 - accuracy: 0.7631\n",
      "Epoch 21/150\n",
      "4543/4543 - 145s - loss: 0.7163 - accuracy: 0.7676\n",
      "Epoch 22/150\n",
      "4543/4543 - 144s - loss: 0.7052 - accuracy: 0.7707\n",
      "Epoch 23/150\n",
      "4543/4543 - 144s - loss: 0.6954 - accuracy: 0.7741\n",
      "Epoch 24/150\n",
      "4543/4543 - 144s - loss: 0.6855 - accuracy: 0.7769\n",
      "Epoch 25/150\n",
      "4543/4543 - 146s - loss: 0.6751 - accuracy: 0.7803\n",
      "Epoch 26/150\n",
      "4543/4543 - 144s - loss: 0.6674 - accuracy: 0.7824\n",
      "Epoch 27/150\n",
      "4543/4543 - 145s - loss: 0.6587 - accuracy: 0.7859\n",
      "Epoch 28/150\n",
      "4543/4543 - 146s - loss: 0.6501 - accuracy: 0.7880\n",
      "Epoch 29/150\n",
      "4543/4543 - 144s - loss: 0.6424 - accuracy: 0.7902\n",
      "Epoch 30/150\n",
      "4543/4543 - 144s - loss: 0.6371 - accuracy: 0.7925\n",
      "Epoch 31/150\n",
      "4543/4543 - 144s - loss: 0.6292 - accuracy: 0.7951\n",
      "Epoch 32/150\n",
      "4543/4543 - 144s - loss: 0.6229 - accuracy: 0.7968\n",
      "Epoch 33/150\n",
      "4543/4543 - 145s - loss: 0.6158 - accuracy: 0.7993\n",
      "Epoch 34/150\n",
      "4543/4543 - 144s - loss: 0.6099 - accuracy: 0.8008\n",
      "Epoch 35/150\n",
      "4543/4543 - 145s - loss: 0.6040 - accuracy: 0.8029\n",
      "Epoch 36/150\n",
      "4543/4543 - 145s - loss: 0.5989 - accuracy: 0.8041\n",
      "Epoch 37/150\n",
      "4543/4543 - 144s - loss: 0.5929 - accuracy: 0.8067\n",
      "Epoch 38/150\n",
      "4543/4543 - 146s - loss: 0.5859 - accuracy: 0.8090\n",
      "Epoch 39/150\n",
      "4543/4543 - 144s - loss: 0.5809 - accuracy: 0.8110\n",
      "Epoch 40/150\n",
      "4543/4543 - 144s - loss: 0.5756 - accuracy: 0.8121\n",
      "Epoch 41/150\n",
      "4543/4543 - 145s - loss: 0.5721 - accuracy: 0.8137\n",
      "Epoch 42/150\n",
      "4543/4543 - 145s - loss: 0.5658 - accuracy: 0.8153\n",
      "Epoch 43/150\n",
      "4543/4543 - 145s - loss: 0.5611 - accuracy: 0.8167\n",
      "Epoch 44/150\n",
      "4543/4543 - 145s - loss: 0.5548 - accuracy: 0.8197\n",
      "Epoch 45/150\n",
      "4543/4543 - 144s - loss: 0.5517 - accuracy: 0.8196\n",
      "Epoch 46/150\n",
      "4543/4543 - 144s - loss: 0.5467 - accuracy: 0.8216\n",
      "Epoch 47/150\n",
      "4543/4543 - 145s - loss: 0.5419 - accuracy: 0.8239\n",
      "Epoch 48/150\n",
      "4543/4543 - 144s - loss: 0.5369 - accuracy: 0.8252\n",
      "Epoch 49/150\n",
      "4543/4543 - 145s - loss: 0.5333 - accuracy: 0.8259\n",
      "Epoch 50/150\n",
      "4543/4543 - 144s - loss: 0.5294 - accuracy: 0.8276\n",
      "Epoch 51/150\n",
      "4543/4543 - 145s - loss: 0.5247 - accuracy: 0.8292\n",
      "Epoch 52/150\n",
      "4543/4543 - 145s - loss: 0.5199 - accuracy: 0.8305\n",
      "Epoch 53/150\n",
      "4543/4543 - 145s - loss: 0.5164 - accuracy: 0.8318\n",
      "Epoch 54/150\n",
      "4543/4543 - 145s - loss: 0.5121 - accuracy: 0.8329\n",
      "Epoch 55/150\n",
      "4543/4543 - 144s - loss: 0.5080 - accuracy: 0.8345\n",
      "Epoch 56/150\n",
      "4543/4543 - 145s - loss: 0.5047 - accuracy: 0.8353\n",
      "Epoch 57/150\n",
      "4543/4543 - 144s - loss: 0.4995 - accuracy: 0.8371\n",
      "Epoch 58/150\n",
      "4543/4543 - 144s - loss: 0.4958 - accuracy: 0.8391\n",
      "Epoch 59/150\n",
      "4543/4543 - 144s - loss: 0.4934 - accuracy: 0.8389\n",
      "Epoch 60/150\n",
      "4543/4543 - 144s - loss: 0.4893 - accuracy: 0.8399\n",
      "Epoch 61/150\n",
      "4543/4543 - 144s - loss: 0.4858 - accuracy: 0.8415\n",
      "Epoch 62/150\n",
      "4543/4543 - 145s - loss: 0.4821 - accuracy: 0.8426\n",
      "Epoch 63/150\n",
      "4543/4543 - 145s - loss: 0.4779 - accuracy: 0.8438\n",
      "Epoch 64/150\n",
      "4543/4543 - 144s - loss: 0.4748 - accuracy: 0.8444\n",
      "Epoch 65/150\n",
      "4543/4543 - 144s - loss: 0.4716 - accuracy: 0.8454\n",
      "Epoch 66/150\n",
      "4543/4543 - 145s - loss: 0.4685 - accuracy: 0.8463\n",
      "Epoch 67/150\n",
      "4543/4543 - 144s - loss: 0.4643 - accuracy: 0.8480\n",
      "Epoch 68/150\n",
      "4543/4543 - 144s - loss: 0.4615 - accuracy: 0.8491\n",
      "Epoch 69/150\n",
      "4543/4543 - 144s - loss: 0.4568 - accuracy: 0.8502\n",
      "Epoch 70/150\n",
      "4543/4543 - 144s - loss: 0.4542 - accuracy: 0.8509\n",
      "Epoch 71/150\n",
      "4543/4543 - 144s - loss: 0.4512 - accuracy: 0.8527\n",
      "Epoch 72/150\n",
      "4543/4543 - 145s - loss: 0.4465 - accuracy: 0.8532\n",
      "Epoch 73/150\n",
      "4543/4543 - 144s - loss: 0.4441 - accuracy: 0.8543\n",
      "Epoch 74/150\n",
      "4543/4543 - 145s - loss: 0.4409 - accuracy: 0.8552\n",
      "Epoch 75/150\n",
      "4543/4543 - 145s - loss: 0.4372 - accuracy: 0.8562\n",
      "Epoch 76/150\n",
      "4543/4543 - 145s - loss: 0.4343 - accuracy: 0.8567\n",
      "Epoch 77/150\n",
      "4543/4543 - 144s - loss: 0.4314 - accuracy: 0.8579\n",
      "Epoch 78/150\n",
      "4543/4543 - 144s - loss: 0.4280 - accuracy: 0.8587\n",
      "Epoch 79/150\n",
      "4543/4543 - 144s - loss: 0.4262 - accuracy: 0.8588\n",
      "Epoch 80/150\n",
      "4543/4543 - 145s - loss: 0.4235 - accuracy: 0.8596\n",
      "Epoch 81/150\n",
      "4543/4543 - 144s - loss: 0.4193 - accuracy: 0.8608\n",
      "Epoch 82/150\n",
      "4543/4543 - 145s - loss: 0.4169 - accuracy: 0.8616\n",
      "Epoch 83/150\n",
      "4543/4543 - 144s - loss: 0.4149 - accuracy: 0.8620\n",
      "Epoch 84/150\n",
      "4543/4543 - 144s - loss: 0.4111 - accuracy: 0.8634\n",
      "Epoch 85/150\n",
      "4543/4543 - 144s - loss: 0.4090 - accuracy: 0.8637\n",
      "Epoch 86/150\n",
      "4543/4543 - 145s - loss: 0.4047 - accuracy: 0.8652\n",
      "Epoch 87/150\n",
      "4543/4543 - 144s - loss: 0.4022 - accuracy: 0.8651\n",
      "Epoch 88/150\n",
      "4543/4543 - 145s - loss: 0.4003 - accuracy: 0.8654\n",
      "Epoch 89/150\n",
      "4543/4543 - 145s - loss: 0.3970 - accuracy: 0.8671\n",
      "Epoch 90/150\n",
      "4543/4543 - 144s - loss: 0.3951 - accuracy: 0.8679\n",
      "Epoch 91/150\n",
      "4543/4543 - 145s - loss: 0.3914 - accuracy: 0.8686\n",
      "Epoch 92/150\n",
      "4543/4543 - 144s - loss: 0.3892 - accuracy: 0.8687\n",
      "Epoch 93/150\n",
      "4543/4543 - 145s - loss: 0.3870 - accuracy: 0.8706\n",
      "Epoch 94/150\n",
      "4543/4543 - 145s - loss: 0.3843 - accuracy: 0.8712\n",
      "Epoch 95/150\n",
      "4543/4543 - 146s - loss: 0.3810 - accuracy: 0.8728\n",
      "Epoch 96/150\n",
      "4543/4543 - 144s - loss: 0.3792 - accuracy: 0.8744\n",
      "Epoch 97/150\n",
      "4543/4543 - 145s - loss: 0.3764 - accuracy: 0.8753\n",
      "Epoch 98/150\n",
      "4543/4543 - 144s - loss: 0.3742 - accuracy: 0.8755\n",
      "Epoch 99/150\n",
      "4543/4543 - 145s - loss: 0.3720 - accuracy: 0.8776\n",
      "Epoch 100/150\n",
      "4543/4543 - 145s - loss: 0.3707 - accuracy: 0.8776\n",
      "Epoch 101/150\n",
      "4543/4543 - 145s - loss: 0.3688 - accuracy: 0.8788\n",
      "Epoch 102/150\n",
      "4543/4543 - 144s - loss: 0.3661 - accuracy: 0.8794\n",
      "Epoch 103/150\n",
      "4543/4543 - 145s - loss: 0.3641 - accuracy: 0.8809\n",
      "Epoch 104/150\n",
      "4543/4543 - 145s - loss: 0.3616 - accuracy: 0.8818\n",
      "Epoch 105/150\n",
      "4543/4543 - 144s - loss: 0.3606 - accuracy: 0.8823\n",
      "Epoch 106/150\n",
      "4543/4543 - 145s - loss: 0.3585 - accuracy: 0.8829\n",
      "Epoch 107/150\n",
      "4543/4543 - 145s - loss: 0.3571 - accuracy: 0.8838\n",
      "Epoch 108/150\n",
      "4543/4543 - 145s - loss: 0.3540 - accuracy: 0.8844\n",
      "Epoch 109/150\n",
      "4543/4543 - 145s - loss: 0.3526 - accuracy: 0.8852\n",
      "Epoch 110/150\n",
      "4543/4543 - 144s - loss: 0.3507 - accuracy: 0.8853\n",
      "Epoch 111/150\n",
      "4543/4543 - 144s - loss: 0.3498 - accuracy: 0.8875\n",
      "Epoch 112/150\n",
      "4543/4543 - 144s - loss: 0.3463 - accuracy: 0.8875\n",
      "Epoch 113/150\n",
      "4543/4543 - 144s - loss: 0.3463 - accuracy: 0.8867\n",
      "Epoch 114/150\n",
      "4543/4543 - 145s - loss: 0.3453 - accuracy: 0.8869\n",
      "Epoch 115/150\n",
      "4543/4543 - 145s - loss: 0.3425 - accuracy: 0.8876\n",
      "Epoch 116/150\n",
      "4543/4543 - 145s - loss: 0.3402 - accuracy: 0.8888\n",
      "Epoch 117/150\n",
      "4543/4543 - 145s - loss: 0.3396 - accuracy: 0.8890\n",
      "Epoch 118/150\n",
      "4543/4543 - 145s - loss: 0.3388 - accuracy: 0.8890\n",
      "Epoch 119/150\n",
      "4543/4543 - 144s - loss: 0.3361 - accuracy: 0.8908\n",
      "Epoch 120/150\n",
      "4543/4543 - 145s - loss: 0.3348 - accuracy: 0.8898\n",
      "Epoch 121/150\n",
      "4543/4543 - 144s - loss: 0.3336 - accuracy: 0.8909\n",
      "Epoch 122/150\n",
      "4543/4543 - 145s - loss: 0.3317 - accuracy: 0.8917\n",
      "Epoch 123/150\n",
      "4543/4543 - 145s - loss: 0.3309 - accuracy: 0.8915\n",
      "Epoch 124/150\n",
      "4543/4543 - 144s - loss: 0.3291 - accuracy: 0.8924\n",
      "Epoch 125/150\n",
      "4543/4543 - 144s - loss: 0.3271 - accuracy: 0.8934\n",
      "Epoch 126/150\n",
      "4543/4543 - 144s - loss: 0.3265 - accuracy: 0.8935\n",
      "Epoch 127/150\n",
      "4543/4543 - 145s - loss: 0.3259 - accuracy: 0.8935\n",
      "Epoch 128/150\n",
      "4543/4543 - 144s - loss: 0.3238 - accuracy: 0.8944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/150\n",
      "4543/4543 - 144s - loss: 0.3233 - accuracy: 0.8940\n",
      "Epoch 130/150\n",
      "4543/4543 - 144s - loss: 0.3199 - accuracy: 0.8950\n",
      "Epoch 131/150\n",
      "4543/4543 - 145s - loss: 0.3208 - accuracy: 0.8952\n",
      "Epoch 132/150\n",
      "4543/4543 - 145s - loss: 0.3186 - accuracy: 0.8962\n",
      "Epoch 133/150\n",
      "4543/4543 - 145s - loss: 0.3176 - accuracy: 0.8964\n",
      "Epoch 134/150\n",
      "4543/4543 - 145s - loss: 0.3159 - accuracy: 0.8970\n",
      "Epoch 135/150\n",
      "4543/4543 - 145s - loss: 0.3160 - accuracy: 0.8963\n",
      "Epoch 136/150\n",
      "4543/4543 - 145s - loss: 0.3133 - accuracy: 0.8972\n",
      "Epoch 137/150\n",
      "4543/4543 - 144s - loss: 0.3120 - accuracy: 0.8982\n",
      "Epoch 138/150\n",
      "4543/4543 - 145s - loss: 0.3123 - accuracy: 0.8981\n",
      "Epoch 139/150\n",
      "4543/4543 - 145s - loss: 0.3110 - accuracy: 0.8982\n",
      "Epoch 140/150\n",
      "4543/4543 - 145s - loss: 0.3096 - accuracy: 0.8987\n",
      "Epoch 141/150\n",
      "4543/4543 - 145s - loss: 0.3086 - accuracy: 0.8989\n",
      "Epoch 142/150\n",
      "4543/4543 - 145s - loss: 0.3080 - accuracy: 0.8993\n",
      "Epoch 143/150\n",
      "4543/4543 - 144s - loss: 0.3075 - accuracy: 0.8985\n",
      "Epoch 144/150\n",
      "4543/4543 - 144s - loss: 0.3055 - accuracy: 0.9003\n",
      "Epoch 145/150\n",
      "4543/4543 - 145s - loss: 0.3039 - accuracy: 0.9004\n",
      "Epoch 146/150\n",
      "4543/4543 - 144s - loss: 0.3036 - accuracy: 0.9004\n",
      "Epoch 147/150\n",
      "4543/4543 - 145s - loss: 0.3021 - accuracy: 0.9009\n",
      "Epoch 148/150\n",
      "4543/4543 - 145s - loss: 0.3014 - accuracy: 0.9014\n",
      "Epoch 149/150\n",
      "4543/4543 - 145s - loss: 0.3004 - accuracy: 0.9023\n",
      "Epoch 150/150\n",
      "4543/4543 - 145s - loss: 0.3001 - accuracy: 0.9021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe7ad061cd0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM_3D(file[\"data/train/gaf\"][0:10],num_of_classes,lr= 0.00002)\n",
    "\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    verbose=2,\n",
    "    epochs=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "LE9cd_A9KwzC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /workspace/notebooks/models/gen/model_gen/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /workspace/notebooks/models/gen/model_gen/assets\n"
     ]
    }
   ],
   "source": [
    "path_model1 = path+\"/models/gen/model_gen\"\n",
    "model.save(path_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(path_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 5, 100, 100, 1)]  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 5, 100, 100, 16)   800       \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 5, 100, 100, 16)   12560     \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 5, 50, 50, 16)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, 5, 50, 50, 16)     2320      \n",
      "_________________________________________________________________\n",
      "time_distributed_28 (TimeDis (None, 5, 50, 50, 16)     2320      \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, 5, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 5, 25, 25, 16)     2320      \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, 5, 25, 25, 16)     2320      \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, 5, 13, 13, 16)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 13, 13, 16)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, 5, 13, 13, 16)     2320      \n",
      "_________________________________________________________________\n",
      "time_distributed_34 (TimeDis (None, 5, 13, 13, 16)     2320      \n",
      "_________________________________________________________________\n",
      "time_distributed_35 (TimeDis (None, 5, 2704)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 16)                174144    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 22)                374       \n",
      "=================================================================\n",
      "Total params: 201,798\n",
      "Trainable params: 201,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run me!\n",
    "from typing import Tuple, Union, Optional\n",
    "\n",
    "import h5py as h5\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from albumentations import Compose\n",
    "import numpy as np\n",
    "\n",
    "available_modes = {\"train\", \"test\"}\n",
    "available_labels_encoding = {\"hot\", \"smooth\", False}\n",
    "\n",
    "\n",
    "class HDF5ImageGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Just a simple custom Keras HDF5 ImageDataGenerator.\n",
    "    \n",
    "    Custom Keras ImageDataGenerator that generates\n",
    "    batches of tensor images from HDF5 files with (optional) real-time\n",
    "    data augmentation.\n",
    "     \n",
    "    Arguments\n",
    "    ---------\n",
    "    src : str\n",
    "        Path of the hdf5 source file.\n",
    "    X_key : str\n",
    "        Key of the h5 file image tensors dataset.\n",
    "        Default is \"images\".\n",
    "    y_key : str\n",
    "        Key of the h5 file labels dataset.\n",
    "        Default is \"labels\".\n",
    "    classes_key : str\n",
    "        Key of the h5 file dataset containing\n",
    "        the raw classes.\n",
    "        Default is None.\n",
    "    batch_size : int\n",
    "        Size of each batch, must be a power of two.\n",
    "        (16, 32, 64, 128, 256, ...)\n",
    "        Default is 32.\n",
    "    shuffle : bool\n",
    "        Shuffle images at the end of each epoch.\n",
    "        Default is True.\n",
    "    scaler : \"std\", \"norm\" or False\n",
    "        \"std\" mode means standardization to range [-1, 1]\n",
    "        with 0 mean and unit variance.\n",
    "        \"norm\" mode means normalization to range [0, 1].\n",
    "        Default is \"std\".\n",
    "    num_classes : None or int\n",
    "        Specifies the total number of classes\n",
    "        for labels encoding.\n",
    "        Default is None.\n",
    "    labels_encoding : \"hot\", \"smooth\" or False\n",
    "        \"hot\" mode means classic one hot encoding.\n",
    "        \"smooth\" mode means smooth hot encoding.\n",
    "        Default is \"hot\".\n",
    "    smooth_factor : int or float\n",
    "        smooth factor used by smooth\n",
    "        labels encoding.\n",
    "        Default is 0.1.\n",
    "    augmenter : albumentations Compose([]) Pipeline or False\n",
    "        An albumentations transformations pipeline\n",
    "        to apply to each sample.\n",
    "        Default is False.\n",
    "    mode : str \"train\" or \"test\"\n",
    "        Model generator type. \"train\" is used for\n",
    "        fit_generator() and evaluate_generator.\n",
    "        \"test\" is used for predict_generator().\n",
    "        Default is \"train\".\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Turn off scaler (scaler=False) if using the\n",
    "    ToFloat(max_value=255) transformation from\n",
    "    albumentations.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    Example of usage:\n",
    "    ```python\n",
    "    my_augmenter = Compose([\n",
    "        HorizontalFlip(p=0.5),\n",
    "        RandomContrast(limit=0.2, p=0.5),\n",
    "        RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "        RandomBrightness(limit=0.2, p=0.5),\n",
    "        Resize(227, 227, cv2.INTER_AREA)\n",
    "    ])\n",
    "    # Create the generator.\n",
    "    train_gen = HDF5ImageGenerator(\n",
    "        'path/to/my/file.h5',\n",
    "         augmenter=my_augmenter)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        src,\n",
    "        X_key=\"data/gasf\",\n",
    "        y_key=\"labels/gaf\",\n",
    "        classes_key=\"classes/appliances\",\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        scaler=True,\n",
    "        num_classes=9,\n",
    "        labels_encoding=\"hot\",\n",
    "        smooth_factor=0.1,\n",
    "        augmenter=False,\n",
    "        mode=\"train\",\n",
    "    ):\n",
    "\n",
    "        if mode not in available_modes:\n",
    "            raise ValueError('`mode` should be `train` '\n",
    "                             '(fit_generator() and evaluate_generator()) or '\n",
    "                             '`test` (predict_generator(). '\n",
    "                             'Received: %s' % mode)\n",
    "        self.mode = mode\n",
    "\n",
    "        if labels_encoding not in available_labels_encoding:\n",
    "            raise ValueError('`labels_encoding` should be `hot` '\n",
    "                             '(classic binary matrix) or '\n",
    "                             '`smooth` (smooth encoding) or '\n",
    "                             'False (no labels encoding). '\n",
    "                             'Received: %s' % labels_encoding)\n",
    "        self.labels_encoding = labels_encoding\n",
    "\n",
    "        if (self.labels_encoding == \"smooth\") and not (0 < smooth_factor <= 1):\n",
    "            raise ValueError('`smooth` labels encoding '\n",
    "                             'must use a `smooth_factor` '\n",
    "                             '< 0 smooth_factor <= 1')\n",
    "\n",
    "        if augmenter and not isinstance(augmenter, Compose):\n",
    "            raise ValueError('`augmenter` argument '\n",
    "                             'must be an instance of albumentations '\n",
    "                             '`Compose` class. '\n",
    "                             'Received type: %s' % type(augmenter))\n",
    "        self.augmenter = augmenter\n",
    "\n",
    "        self.src: str = src\n",
    "        self.X_key: str = X_key\n",
    "        self.y_key: str = y_key\n",
    "        self.classes_key: str = classes_key\n",
    "        self.batch_size: int = batch_size\n",
    "        self.shuffle: bool = shuffle\n",
    "        self.scaler: bool = scaler\n",
    "        self.num_classes: int = num_classes\n",
    "        self.smooth_factor: float = smooth_factor\n",
    "\n",
    "        self._indices = np.arange(self.__get_dataset_shape(self.X_key, 0))\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Representation of the class.\"\"\"\n",
    "        return f\"{self.__class__.__name__}({self.__dict__!r})\"\n",
    "\n",
    "    def __get_dataset_shape(self, dataset: str, index: int) -> Tuple[int, ...]:\n",
    "        \"\"\"Get an h5py dataset shape.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        dataset : str\n",
    "            The dataset key.\n",
    "        index : int\n",
    "            The dataset index.\n",
    "         \n",
    "        Returns\n",
    "        -------\n",
    "        tuple of ints\n",
    "            A tuple of array dimensions.\n",
    "        \"\"\"\n",
    "        with h5.File(self.src, \"r\", libver=\"latest\", swmr=True) as file:\n",
    "            return file[dataset].shape[index]\n",
    "\n",
    "    def __get_dataset_items(\n",
    "        self,\n",
    "        indices: np.ndarray,\n",
    "        dataset: Optional[str] = None\n",
    "    ) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Get an HDF5 dataset items.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        indices : ndarray, \n",
    "            The list of current batch indices.\n",
    "        dataset : (optional) str\n",
    "            The dataset key. If None, returns\n",
    "            a batch of (image tensors, labels).\n",
    "            Defaults to None.\n",
    "         \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray or a tuple of ndarrays\n",
    "            A batch of samples.\n",
    "        \"\"\"\n",
    "        with h5.File(self.src, \"r\", libver=\"latest\", swmr=True) as file:\n",
    "            if dataset is not None:\n",
    "                return file[dataset][indices]\n",
    "            else:\n",
    "                return (file[self.X_key][indices], file[self.y_key][indices])\n",
    "    \n",
    "    @property\n",
    "    def num_items(self) -> int:\n",
    "        \"\"\"Grab the total number of examples\n",
    "         from the dataset.\n",
    "         \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The total number of examples.\n",
    "        \"\"\"\n",
    "        with h5.File(self.src, \"r\") as file:\n",
    "            return file[self.X_key].shape[0]\n",
    "    \n",
    "    @property \n",
    "    def classes(self) -> list:\n",
    "        \"\"\"Grab \"human\" classes from the dataset.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            A list of the raw classes.\n",
    "        \"\"\"\n",
    "        if self.classes_key is None:\n",
    "            raise ValueError('Canceled. parameter `classes_key` '\n",
    "                             'is set to None.')\n",
    "        \n",
    "        with h5.File(self.src, \"r\") as file:\n",
    "            return file[self.classes_key][:]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch.\n",
    "         \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of batches per epochs.\n",
    "        \"\"\"\n",
    "        return int(\n",
    "            np.ceil(\n",
    "                self.__get_dataset_shape(self.X_key, 0) /\n",
    "                float(self.batch_size)))\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_labels_smoothing(batch_y: np.ndarray,\n",
    "                               factor: float) -> np.ndarray:\n",
    "        \"\"\"Applies labels smoothing to the original\n",
    "         labels binary matrix.\n",
    "         \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_y : np.ndarray\n",
    "            Current batch integer labels.\n",
    "        factor : float\n",
    "            Smoothing factor.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            A binary class matrix.\n",
    "        \"\"\"\n",
    "        batch_y *= 1 - factor\n",
    "        batch_y += factor / batch_y.shape[1]\n",
    "\n",
    "        return batch_y\n",
    "\n",
    "    def apply_labels_encoding(\n",
    "            self,\n",
    "            batch_y: np.ndarray,\n",
    "            smooth_factor: Optional[float] = None) -> np.ndarray:\n",
    "        \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "         See Keras to_categorical utils function.\n",
    "         \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_y : np.ndarray\n",
    "            Current batch integer labels.\n",
    "        smooth_factor : (optional) Float\n",
    "            Smooth factor.\n",
    "            Defaults to None.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            A binary class matrix.\n",
    "        \"\"\"\n",
    "        batch_y = to_categorical(batch_y, num_classes=self.num_classes)\n",
    "\n",
    "        if smooth_factor is not None:\n",
    "            batch_y = self.apply_labels_smoothing(batch_y,\n",
    "                                                  factor=smooth_factor)\n",
    "\n",
    "        return batch_y\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_normalization(batch_X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize the pixel intensities. \n",
    "        \n",
    "        Normalize the pixel intensities to the range [0, 1].\n",
    "         \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_X : np.ndarray\n",
    "            Batch of image tensors to be normalized.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            A batch of normalized image tensors.\n",
    "        \"\"\"\n",
    "        return batch_X.astype(\"float32\") / 255.0\n",
    "\n",
    "    def __next_batch_test(self, indices: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Generates a batch of test data for the given indices.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        index : int\n",
    "            The index for the batch.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            4D tensor (num_samples, height, width, depth).\n",
    "        \"\"\"\n",
    "        # Grab corresponding images from the HDF5 source file.\n",
    "        batch_X = self.__get_dataset_items(indices, self.X_key)\n",
    "\n",
    "        # Shall we rescale features?\n",
    "        if self.scaler:\n",
    "            batch_X = self.apply_normalization(batch_X)\n",
    "\n",
    "        return batch_X\n",
    "\n",
    "    def __next_batch(self,\n",
    "                     indices: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Generates a batch of train/val data for the given indices.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        index : int\n",
    "            The index for the batch.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple of ndarrays\n",
    "            A tuple containing a batch of image tensors\n",
    "            and their associated labels.\n",
    "        \"\"\"\n",
    "        # Grab samples (tensors, labels) HDF5 source file.\n",
    "        (batch_X, batch_y) = self.__get_dataset_items(indices)\n",
    "\n",
    "        # Shall we apply any data augmentation?\n",
    "        if self.augmenter:\n",
    "            batch_X = np.stack(\n",
    "                [self.augmenter(image=x)[\"image\"] for x in batch_X], axis=0)\n",
    "\n",
    "        # Shall we rescale features?\n",
    "        if self.scaler:\n",
    "            batch_X = self.apply_normalization(batch_X)\n",
    "\n",
    "        # Shall we apply labels encoding?\n",
    "        if self.labels_encoding:\n",
    "            batch_y = self.apply_labels_encoding(\n",
    "                batch_y,\n",
    "                smooth_factor=self.smooth_factor\n",
    "                if self.labels_encoding == \"smooth\" else None,\n",
    "            )\n",
    "\n",
    "        return (batch_X, batch_y)\n",
    "\n",
    "    def __getitem__(\n",
    "            self,\n",
    "            index: int) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Generates a batch of data for the given index.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        index : int\n",
    "            The index for the current batch.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple of ndarrays or ndarray\n",
    "            A tuple containing a batch of image tensors\n",
    "            and their associated labels (train) or\n",
    "            a tuple of image tensors (predict).\n",
    "        \"\"\"\n",
    "        # Indices for the current batch.\n",
    "        indices = np.sort(self._indices[index * self.batch_size:(index + 1) *\n",
    "                                        self.batch_size])\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            return self.__next_batch(indices)\n",
    "        else:\n",
    "            return self.__next_batch_test(indices)\n",
    "\n",
    "    def __shuffle_indices(self):\n",
    "        \"\"\"If the shuffle parameter is set to True,\n",
    "         dataset will be shuffled (in-place).\n",
    "         (not available in test 'mode').\n",
    "        \"\"\"\n",
    "        if (self.mode == \"train\") and self.shuffle:\n",
    "            np.random.shuffle(self._indices)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Triggered once at the very beginning as well as \n",
    "         at the end of each epoch.\n",
    "        \"\"\"\n",
    "        self.__shuffle_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMej4PvP65qsLng7TnQKWA4",
   "name": "LSTM-gen",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
